{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"X_tnmIPfYYhB"},"outputs":[{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Requirement already satisfied: datasets in /Users/ritikojha/Library/Python/3.9/lib/python/site-packages (3.5.0)\n","Requirement already satisfied: filelock in /Users/ritikojha/Library/Python/3.9/lib/python/site-packages (from datasets) (3.17.0)\n","Requirement already satisfied: numpy>=1.17 in /Users/ritikojha/Library/Python/3.9/lib/python/site-packages (from datasets) (1.26.4)\n","Requirement already satisfied: pyarrow>=15.0.0 in /Users/ritikojha/Library/Python/3.9/lib/python/site-packages (from datasets) (18.0.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/ritikojha/Library/Python/3.9/lib/python/site-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /Users/ritikojha/Library/Python/3.9/lib/python/site-packages (from datasets) (2.2.3)\n","Requirement already satisfied: requests>=2.32.2 in /Users/ritikojha/Library/Python/3.9/lib/python/site-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /Users/ritikojha/Library/Python/3.9/lib/python/site-packages (from datasets) (4.66.6)\n","Requirement already satisfied: xxhash in /Users/ritikojha/Library/Python/3.9/lib/python/site-packages (from datasets) (3.5.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /Users/ritikojha/Library/Python/3.9/lib/python/site-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /Users/ritikojha/Library/Python/3.9/lib/python/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n","Requirement already satisfied: aiohttp in /Users/ritikojha/Library/Python/3.9/lib/python/site-packages (from datasets) (3.11.7)\n","Requirement already satisfied: huggingface-hub>=0.24.0 in /Users/ritikojha/Library/Python/3.9/lib/python/site-packages (from datasets) (0.29.3)\n","Requirement already satisfied: packaging in /Users/ritikojha/Library/Python/3.9/lib/python/site-packages (from datasets) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /Users/ritikojha/Library/Python/3.9/lib/python/site-packages (from datasets) (6.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/ritikojha/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets) (2.4.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /Users/ritikojha/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: async-timeout<6.0,>=4.0 in /Users/ritikojha/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: attrs>=17.3.0 in /Users/ritikojha/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /Users/ritikojha/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /Users/ritikojha/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /Users/ritikojha/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets) (0.2.0)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/ritikojha/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets) (1.18.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/ritikojha/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ritikojha/Library/Python/3.9/lib/python/site-packages (from requests>=2.32.2->datasets) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /Users/ritikojha/Library/Python/3.9/lib/python/site-packages (from requests>=2.32.2->datasets) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ritikojha/Library/Python/3.9/lib/python/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /Users/ritikojha/Library/Python/3.9/lib/python/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /Users/ritikojha/Library/Python/3.9/lib/python/site-packages (from pandas->datasets) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /Users/ritikojha/Library/Python/3.9/lib/python/site-packages (from pandas->datasets) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /Users/ritikojha/Library/Python/3.9/lib/python/site-packages (from pandas->datasets) (2024.2)\n","Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.15.0)\n","\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install datasets"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":393},"executionInfo":{"elapsed":15,"status":"error","timestamp":1746671033124,"user":{"displayName":"Ritik Ojha","userId":"08390397347787036077"},"user_tz":240},"id":"TOaOaDP8HMa7","outputId":"80dc8b18-9539-4456-e765-94b92fa813aa"},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/ritikojha/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Cleaned subset shape: (193274, 2)\n","Subset shape: (193274, 2)\n"]}],"source":["from datasets import load_dataset\n","import pandas as pd\n","import numpy as  np\n","import re\n","\n","# Load the dataset and select the first 100,000 rows, then take a subset of 1,000\n","ds = load_dataset(\"MohammadOthman/mo-customer-support-tweets-945k\")\n","data = ds['train'].select(range(200000)).to_pandas()\n","# data_subset = data.head(1000)  # Subset for demonstration\n","data_subset = data\n","\n","####################################\n","# Drop rows with NaN values in 'input' or 'output'\n","data_subset.dropna(subset=['input', 'output'], inplace=True)\n","\n","# Drop rows with empty strings or only one word in either 'input' or 'output'\n","def is_valid(text):\n","    return isinstance(text, str) and len(text.strip().split()) > 1\n","\n","data_subset = data_subset[data_subset['input'].apply(is_valid)]\n","data_subset = data_subset[data_subset['output'].apply(is_valid)]\n","\n","# Reset index after filtering\n","data_subset.reset_index(drop=True, inplace=True)\n","\n","print(\"Cleaned subset shape:\", data_subset.shape)\n","############################################################\n","\n","\n","# Preprocess text: lowercase and remove special characters\n","def preprocess_text(text):\n","    if not isinstance(text, str):  # Handle non-string inputs\n","        return \"\"\n","    text = text.lower()\n","    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n","    return text.strip()\n","\n","# Apply preprocessing to input and output columns\n","data_subset['input'] = data_subset['input'].apply(preprocess_text)\n","data_subset['output'] = data_subset['output'].apply(preprocess_text)\n","\n","print(\"Subset shape:\", data_subset.shape)"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"NRmDgsPGHMa8"},"outputs":[],"source":["# data_subset[['output', 'input']].to_csv('main_data.csv', index = False)"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"TOzOL6fmHMa9"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>output</th>\n","      <th>input</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>i would love the chance to review the account ...</td>\n","      <td>is the worst customer service</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>h there we would definitely like to work with ...</td>\n","      <td>yall lie about your great connection 5 bars lt...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>we understand your concerns and we would like ...</td>\n","      <td>since i signed up with yousince day 1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>this is saddening to hear please shoot us a dm...</td>\n","      <td>you gonna magically change your connectivity f...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>please send me a private message so that i can...</td>\n","      <td>whenever i contact customer support they tell ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                              output  \\\n","0  i would love the chance to review the account ...   \n","1  h there we would definitely like to work with ...   \n","2  we understand your concerns and we would like ...   \n","3  this is saddening to hear please shoot us a dm...   \n","4  please send me a private message so that i can...   \n","\n","                                               input  \n","0                      is the worst customer service  \n","1  yall lie about your great connection 5 bars lt...  \n","2              since i signed up with yousince day 1  \n","3  you gonna magically change your connectivity f...  \n","4  whenever i contact customer support they tell ...  "]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["data_subset.head()"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"gJDivWxJHMa9"},"outputs":[],"source":["\n","# output-- do precessing try to search some metho dto see if the language is other than english, and then i have seen somewhere wghere the words were spell wrong(try some simple method to correct the word)\n","# embeddings--don't do bert, try faster static embeddings from hugging face(or any faster embedding), or try tfidf later on.\n","# clustering on embedding-- kmeans(kmeans will be bad because of higher dimension.., for another option, try reducing dimension by pca, pls, and then on those reduced dimension , do kmeans) try other clustering methods like tsne, umap, try other fast clustering which is good for higher dimension\n","\n","# in future--, for each cluster, we will try to implement something like different kind chatbiot we can develop\n"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"1XpkRX4nK-fm"},"outputs":[],"source":["data_subset['conversation'] = data_subset['input'] + \" \" +data_subset['output']"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["INFO: Pandarallel will run on 11 workers.\n","INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n"]}],"source":["import pandas as pd\n","from langdetect import detect, DetectorFactory\n","from langdetect.lang_detect_exception import LangDetectException\n","from pandarallel import pandarallel\n","\n","# Ensures consistent language detection\n","DetectorFactory.seed = 0\n","\n","# Initialize pandarallel\n","pandarallel.initialize(progress_bar=True)  # Set to False to hide progress bar\n","\n","# Sample large DataFrame (replace this with your actual one)\n","# df = pd.read_csv(\"your_large_file.csv\")\n","# Let's say the relevant column is 'text'\n","# For demo:\n","# df = pd.DataFrame({'text': [...large text list...]})\n","\n","# Function to check if text is English\n","def is_english(text):\n","    try:\n","        return detect(text) == 'en'\n","    except LangDetectException:\n","        return False\n","\n","# Use parallel apply\n","# df['is_english'] = df['text'].parallel_apply(is_english)\n","\n","# # Filter for English rows only\n","# df_english = df[df['is_english']].drop(columns='is_english').reset_index(drop=True)\n","\n","# # Done!\n","# print(df_english.head())\n"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"866690c6944c4968a48b30de773fb64e","version_major":2,"version_minor":0},"text/plain":["VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=17571), Label(value='0 / 17571')))…"]},"metadata":{},"output_type":"display_data"}],"source":["data_subset['is_english'] = data_subset['conversation'].parallel_apply(is_english)\n","\n","# Filter for English rows only\n","data_subset = data_subset[data_subset['is_english']].drop(columns='is_english').reset_index(drop=True)\n","\n","# Done!\n"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/plain":["(185457, 3)"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["data_subset.shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RSaMEazeHMa9"},"outputs":[],"source":["# from transformers import BertTokenizer, BertModel\n","# import torch\n","# import numpy as np\n","\n","# # Load BERT tokenizer and model\n","# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","# model = BertModel.from_pretrained('bert-base-uncased')\n","\n","# # Function to generate BERT embeddings\n","# def get_bert_embedding(text):\n","#     if not text:  # Handle empty strings\n","#         return np.zeros(768)  # Return zero vector for empty text\n","#     inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n","#     with torch.no_grad():\n","#         outputs = model(**inputs)\n","#     # Use the [CLS] token embedding\n","#     return outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n","\n","# # Generate embeddings for agent responses\n","# embeddings_list = [get_bert_embedding(text) for text in data_subset['conversation']]\n","# embeddings_matrix = np.array(embeddings_list)\n","\n","# print(\"Embeddings matrix shape:\", embeddings_matrix.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21266,"status":"ok","timestamp":1746670800915,"user":{"displayName":"Ritik Ojha","userId":"08390397347787036077"},"user_tz":240},"id":"hVVjHQ4y35IS","outputId":"d6aff2d0-bbfa-4418-d8ce-dfa55a5c4bfc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Embedding batches:  85%|████████▍ | 2452/2898 [22:07<04:50,  1.54it/s]"]},{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n","\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n","\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n","\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["from transformers import DistilBertTokenizer, DistilBertModel\n","import torch, numpy as np\n","from torch.utils.data import DataLoader, Dataset\n","from tqdm import tqdm\n","\n","# 1) Setup\n","tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n","model     = DistilBertModel.from_pretrained('distilbert-base-uncased')\n","device    = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device).eval()\n","\n","# 2) Dataset returns raw text\n","class ConversationDataset(Dataset):\n","    def __init__(self, texts):\n","        self.texts = texts\n","    def __len__(self):\n","        return len(self.texts)\n","    def __getitem__(self, idx):\n","        # Return the raw string (empty→\"\")\n","        return self.texts[idx] or \"\"\n","\n","# 3) collate_fn: batch→tokenized, padded tensors\n","def collate_fn(batch_texts):\n","    # batch_texts: list[str]\n","    encoded = tokenizer(\n","        batch_texts,\n","        return_tensors=\"pt\",\n","        padding=True,\n","        truncation=True,\n","        max_length=512\n","    )\n","    # move to device\n","    return {k: v.to(device) for k, v in encoded.items()}\n","\n","# 4) Embedding function\n","@torch.no_grad()\n","def get_bert_embeddings_batch(batch):\n","    outputs = model(**batch)\n","    # take [CLS] token (first token) out, move to CPU numpy\n","    return outputs.last_hidden_state[:, 0, :].cpu().numpy()\n","\n","# 5) DataLoader with custom collate_fn\n","batch_size = 64\n","dataset    = ConversationDataset(data_subset['conversation'].tolist())\n","dataloader = DataLoader(\n","    dataset,\n","    batch_size=batch_size,\n","    shuffle=False,\n","    collate_fn=collate_fn,  # ← ensures uniform padding\n","    num_workers=0           # safe for notebooks\n",")\n","\n","# 6) Generate embeddings\n","emb_list = []\n","for batch in tqdm(dataloader, desc=\"Embedding batches\"):\n","    emb = get_bert_embeddings_batch(batch)\n","    emb_list.append(emb)\n","\n","embeddings_matrix = np.vstack(emb_list)\n","print(\"Embeddings matrix shape:\", embeddings_matrix.shape)\n"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"zLIQSmpSPK4i"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"906b28bf15fd404995a504a77c81470c","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f82383b458414e899e88b04f980a5c53","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c3769ce757c443a18e038cd6c98f4d54","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d8fd5e8f25c347399cb126db34c21c82","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"375878c78ac34bab9814dbaf4d693e0b","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":[]},{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n","\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n","\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n","\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["# # distilbert\n","# # Import required libraries\n","# from transformers import DistilBertTokenizer, DistilBertModel\n","# import torch\n","# import numpy as np\n","\n","# # Load DistilBERT tokenizer and model\n","# tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n","# model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n","\n","# # Move model to GPU if available\n","# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","# model.to(device)\n","# model.eval()\n","\n","# # Function to generate DistilBERT embeddings using GPU\n","# def get_distilbert_embedding(text):\n","#     if not text:\n","#         return np.zeros(768)  # Handle empty or null strings\n","\n","#     # Tokenize and move tensors to device\n","#     inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n","\n","#     with torch.no_grad():\n","#         outputs = model(**inputs)\n","\n","#     # Get the [CLS] token embedding, move back to CPU before converting to NumPy\n","#     cls_embedding = outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()\n","#     return cls_embedding\n","\n","# # Generate embeddings for agent responses with progress bar\n","# from tqdm import tqdm\n","\n","# embeddings_list = [get_distilbert_embedding(text) for text in tqdm(data_subset['conversation'])]\n","\n","# # Convert embeddings list to a NumPy array\n","# embeddings_matrix = np.array(embeddings_list)\n","\n","# # Print the shape of the embeddings matrix\n","# print(\"Embeddings matrix shape:\", embeddings_matrix.shape)"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":598,"referenced_widgets":["deb5f38e66834022856e1796de42b861","6578b02a192a4564bc57810772b4bfbc","1e1742a1a4eb40879590c2b66e3baf1a","3c80dfc7caa44d64a4a21c2204929f9d","48dffd59903a463283b8db0ee4dea8b5","6cffe51a3f144cbe9c59a6d38d61486e","e28cd1ae603149bb8ad8612a0db01bfa","528bd088204a46a48f64e1f23da09a30","d03dc7000e6a4f309e89daa03a2a4c4e","953d3bcac0774397b841abc6d0d3787b","8eb993f0f1174fffbd10869bd406605e","575c731ec8a44d8b8c28c7d471cd5403","47161eb6ea024fd797ed37a8c612b8d5","40e5d3e685c34e5196c52cbd462d6fd7","6c62d15389784891ac271b71366a003c","2bad9f44bf874712abaa177db748184c","8dd5e7105a724ad79bfb49ac0adb78b1","be8a890fe826449c91d5dd82f1cc47f5","cbb007e930c64b24881815c2a7e11a82","9a04f882947b49c6902688885f62c6b3","1a00f4d394d3424a9fb1de404c9d8e19","2070db58e6624c44b8727de08f863cd6","8e89198ae09242d9b8887fddb1a7f969","7de7680032784b2bad2506fbfe7daf2e","81983c0ec470445e82d1bb52f0dc98db","2d0cbd95bb9b42669a9ff3774badb466","5e6302875be544b9a25d2eb9e12a726c","967038c825554ffd814fe90ebb6bee30","7d943efcc86841e2b4901b1d474fd436","38fcd393a390457bbb3e755a32830466","febab2bec880457a85bdcc0d2047d3e7","1b42788ff7354dc7b09069c72008b7ec","72fc63d3fe214a939995de1c7b5a838a","c9a245d2c9e145e1aac36222cffb02c4","e736795d01c243c2bbeab4d951607547","a38268f765514ac8a85b2f8d3281f91b","afcb76ed830a4244a94a3d6b4448cdeb","1820f1a74f9c48ab9f28622ce516836c","949fcb113a28446e899dbccd47bfa6a0","faa69438909d4babaab78a1271505100","f39ab185245f4ea4b6800f33ea34d632","75b0c655fd8b49469e8d220ef0e7e2ec","f513a35e2825439bbbabd794b56987fa","ab486fc72d4640d3b7fd20a794558069","09235973e4d046ec90982e02cbe5a8e0","a6c2286141d14050ab9b79fef1e354f1","e47f66b021164d149d4f3e925e9741eb","d192ebdfd02944538b7163b409386c43","f7b08b27ea7f47e2b0efed3b128c8eb6","d32cbe465c314025ab4bb9ea56680a60","f1a236102d0743ed904a8e2585484df7","04e7f398826944a5b6d1e902b8df831d","c2eff01113024168b02cc6eddd4c9f42","a68ed39eeabf4a42b1c8a3213e4a9973","ffd5fd6bd7584e5798af662d28026212"]},"executionInfo":{"elapsed":9970,"status":"error","timestamp":1745005067822,"user":{"displayName":"Ritik Ojha","userId":"08390397347787036077"},"user_tz":240},"id":"D0_oAzanO7xr","outputId":"4301da6a-9c36-4100-9fc2-7638615a184d"},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/466773 [00:00<?, ?it/s]"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[17], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Generate embeddings for agent responses\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m---> 32\u001b[0m embeddings_list \u001b[38;5;241m=\u001b[39m [get_bert_embedding(text) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m tqdm(data_subset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconversation\u001b[39m\u001b[38;5;124m'\u001b[39m])]\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# embeddings_list = [get_bert_embedding(text) for text in data_subset['conversation']]\u001b[39;00m\n\u001b[1;32m     35\u001b[0m embeddings_matrix \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(embeddings_list)\n","Cell \u001b[0;32mIn[17], line 32\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Generate embeddings for agent responses\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m---> 32\u001b[0m embeddings_list \u001b[38;5;241m=\u001b[39m [\u001b[43mget_bert_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m tqdm(data_subset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconversation\u001b[39m\u001b[38;5;124m'\u001b[39m])]\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# embeddings_list = [get_bert_embedding(text) for text in data_subset['conversation']]\u001b[39;00m\n\u001b[1;32m     35\u001b[0m embeddings_matrix \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(embeddings_list)\n","Cell \u001b[0;32mIn[17], line 23\u001b[0m, in \u001b[0;36mget_bert_embedding\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     20\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 23\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Get the [CLS] token embedding, move back to CPU before converting to NumPy\u001b[39;00m\n\u001b[1;32m     26\u001b[0m cls_embedding \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state[:, \u001b[38;5;241m0\u001b[39m, :]\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n","File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n","File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/models/bert/modeling_bert.py:1142\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m-> 1142\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1154\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1155\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n","File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/models/bert/modeling_bert.py:695\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    684\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    685\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    686\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    692\u001b[0m         output_attentions,\n\u001b[1;32m    693\u001b[0m     )\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 695\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n","File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n","File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/models/bert/modeling_bert.py:627\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    624\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    625\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 627\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    632\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n","File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/pytorch_utils.py:261\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 261\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/models/bert/modeling_bert.py:640\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[1;32m    639\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[0;32m--> 640\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n","File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n","File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/models/bert/modeling_bert.py:552\u001b[0m, in \u001b[0;36mBertOutput.forward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 552\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    553\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m    554\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n","File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n","File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# bert network, big transformer network--\n","\n","# from transformers import BertTokenizer, BertModel\n","# import torch\n","# import numpy as np\n","\n","# # Load BERT tokenizer and model\n","# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","# model = BertModel.from_pretrained('bert-base-uncased')\n","\n","# # Move model to GPU if available\n","# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","# model.to(device)\n","# model.eval()\n","\n","# # Function to generate BERT embeddings using GPU\n","# def get_bert_embedding(text):\n","#     if not text:\n","#         return np.zeros(768)  # Handle empty or null strings\n","\n","#     # Tokenize and move tensors to device\n","#     inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n","\n","#     with torch.no_grad():\n","#         outputs = model(**inputs)\n","\n","#     # Get the [CLS] token embedding, move back to CPU before converting to NumPy\n","#     cls_embedding = outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()\n","#     return cls_embedding\n","\n","# # Generate embeddings for agent responses\n","# from tqdm import tqdm\n","\n","# embeddings_list = [get_bert_embedding(text) for text in tqdm(data_subset['conversation'])]\n","\n","# # embeddings_list = [get_bert_embedding(text) for text in data_subset['conversation']]\n","# embeddings_matrix = np.array(embeddings_list)\n","\n","# print(\"Embeddings matrix shape:\", embeddings_matrix.shape)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UBQ4XDCMHMa9"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PHUav6BPHMa-"},"outputs":[],"source":["# saving and loading embeddings\n","import numpy as np\n","# Save the embeddings matrix to a file\n","np.save(\"/content/drive/MyDrive/project_fdm/bert_embeddings_cluster_collab.npy\", embeddings_matrix)\n","print(\"Embeddings saved to 'bert_embeddings.npy'\")\n"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":341},"executionInfo":{"elapsed":612,"status":"error","timestamp":1746670978431,"user":{"displayName":"Ritik Ojha","userId":"08390397347787036077"},"user_tz":240},"id":"8lQ2J-HhHMa-","outputId":"1c290376-64ac-46ec-a0a2-aed80b9a1ccc"},"outputs":[{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/drive/MyDrive/project_fdm/bert_embeddings_cluster_collab.npy'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-df6f5cc1200c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Load the saved embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mloaded_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/project_fdm/bert_embeddings_cluster_collab.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loaded embeddings shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloaded_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/lib/_npyio_impl.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    453\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/project_fdm/bert_embeddings_cluster_collab.npy'"]}],"source":["# loading embeddings\n","# Load the saved embeddings\n","loaded_embeddings = np.load(\"/content/drive/MyDrive/project_fdm/bert_embeddings_cluster_collab.npy\")\n","print(\"Loaded embeddings shape:\", loaded_embeddings.shape)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Djcet_5gHMa_","outputId":"1e197080-583c-4a52-b6e0-b937b1e9fbba"},"outputs":[{"name":"stdout","output_type":"stream","text":["Best number of clusters (based on silhouette): 2\n"]}],"source":["\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7MIYXd-THMa_"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":298554,"status":"ok","timestamp":1744964126722,"user":{"displayName":"Ritik Ojha","userId":"08390397347787036077"},"user_tz":240},"id":"G8jQCoyiHMa_","outputId":"5646bba8-3d67-4daa-bf2f-e0dc4bf89640"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n","  warn(\n"]},{"name":"stdout","output_type":"stream","text":["UMAP reduced shape: (96391, 50)\n"]}],"source":["import umap.umap_ as umap\n","\n","# Reduce to 50 dimensions with UMAP\n","umap_reducer = umap.UMAP(n_components=50, random_state=42, n_neighbors=15, min_dist=0.1)\n","reduced_embeddings_umap = umap_reducer.fit_transform(embeddings_matrix)\n","\n","print(\"UMAP reduced shape:\", reduced_embeddings_umap.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10308,"status":"ok","timestamp":1744964137029,"user":{"displayName":"Ritik Ojha","userId":"08390397347787036077"},"user_tz":240},"id":"Jq2GRBqCHMa_","outputId":"853df0aa-4da6-4d03-c34a-ef83eb96016b"},"outputs":[{"name":"stdout","output_type":"stream","text":["SVD reduced shape: (96391, 50)\n"]}],"source":["from sklearn.decomposition import TruncatedSVD\n","\n","# Reduce to 50 dimensions with SVD\n","svd = TruncatedSVD(n_components=50, random_state=42)\n","reduced_embeddings_svd = svd.fit_transform(embeddings_matrix)\n","\n","print(\"SVD reduced shape:\", reduced_embeddings_svd.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WyOUx0FRxTWN"},"outputs":[],"source":["from sklearn.manifold import TSNE\n","tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n","reduced_embeddings_tsne = tsne.fit_transform(embeddings_matrix)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CtE67ZGRHMa_"},"outputs":[],"source":["from sklearn.cluster import MiniBatchKMeans\n","\n","# On UMAP-reduced data\n","minibatch_kmeans_umap = MiniBatchKMeans(n_clusters=4, random_state=42, batch_size=100)\n","clusters_minibatch_umap = minibatch_kmeans_umap.fit_predict(reduced_embeddings_umap)\n","\n","# On SVD-reduced data\n","minibatch_kmeans_svd = MiniBatchKMeans(n_clusters=4, random_state=42, batch_size=100)\n","clusters_minibatch_svd = minibatch_kmeans_svd.fit_predict(reduced_embeddings_svd)\n","\n","minibatch_kmeans_tsne = MiniBatchKMeans(n_clusters=4, random_state=42, batch_size=100)\n","clusters_minibatch_tsne = minibatch_kmeans_tsne.fit_predict(reduced_embeddings_tsne)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TgXx9IXe3hr9"},"outputs":[],"source":["from sklearn.cluster import KMeans\n","\n","# On UMAP-reduced data\n","kmeans_umap = KMeans(n_clusters=4, random_state=42)\n","clusters_kmeans_umap = kmeans_umap.fit_predict(reduced_embeddings_umap)\n","\n","# On SVD-reduced data\n","# kmeans_svd = KMeans(n_clusters=4, random_state=42)\n","# clusters_kmeans_svd = kmeans_svd.fit_predict(reduced_embeddings_svd)\n","\n","# # On tsne-reduced data\n","# kmeans_tsne = KMeans(n_clusters=4, random_state=42)\n","# clusters_kmeans_tsne = kmeans_tsne.fit_predict(reduced_embeddings_tsne)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gaMySYKKY7vP"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","import numpy as np\n","\n","# Assuming these exist:\n","# reduced_embeddings_umap: 2D numpy array from UMAP\n","# clusters_kmeans_umap: predicted cluster labels from KMeans\n","\n","# Number of clusters\n","num_clusters = len(np.unique(clusters_kmeans_umap))\n","\n","# Set Seaborn style\n","sns.set(style=\"white\", context=\"notebook\")\n","\n","# Create the plot\n","plt.figure(figsize=(10, 7))\n","scatter = plt.scatter(\n","    reduced_embeddings_umap[:, 0],\n","    reduced_embeddings_umap[:, 1],\n","    c=clusters_kmeans_umap,\n","    cmap=plt.cm.get_cmap('tab20', num_clusters),\n","    s=60,\n","    alpha=0.8,\n","    linewidths=0.3,\n","    edgecolors='gray'\n",")\n","\n","# Legend with cluster IDs\n","legend1 = plt.legend(*scatter.legend_elements(num=num_clusters), title=\"Cluster\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n","plt.gca().add_artist(legend1)\n","\n","# Clean axis and layout\n","sns.despine()\n","plt.xlabel(\"UMAP Dimension 1\", fontsize=12)\n","plt.ylabel(\"UMAP Dimension 2\", fontsize=12)\n","plt.title(\"Enhanced KMeans Clustering on UMAP-Reduced Embeddings\", fontsize=14)\n","plt.grid(False)\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lcsPRs1WY7sh"},"outputs":[],"source":["# cluster ceenter\n","# Optional: If you have the fitted KMeans object\n","centers = kmeans_umap.cluster_centers_  # from the fitted KMeans on UMAP embeddings\n","plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.7, marker='X', label='Centers')\n","plt.legend()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_kjuevCy3hpF"},"outputs":[],"source":["from sklearn.cluster import DBSCAN\n","\n","# On UMAP-reduced data\n","dbscan_umap = DBSCAN(eps=0.5, min_samples=5)\n","clusters_dbscan_umap = dbscan_umap.fit_predict(reduced_embeddings_umap)\n","\n","# On SVD-reduced data\n","dbscan_svd = DBSCAN(eps=0.5, min_samples=5)\n","clusters_dbscan_svd = dbscan_svd.fit_predict(reduced_embeddings_svd)\n","\n","dbscan_tsne = DBSCAN(eps=0.5, min_samples=5)\n","clusters_dbscan_tsne = dbscan_tsne.fit_predict(reduced_embeddings_tsne)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1174035,"status":"ok","timestamp":1744967540710,"user":{"displayName":"Ritik Ojha","userId":"08390397347787036077"},"user_tz":240},"id":"p2yiIt6i3hja","outputId":"eb2fafbe-4b06-4b8d-d8e2-72da3dec1222"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mini-Batch K-Means (UMAP): Silhouette Score = 0.3382, Clusters = 4\n","Mini-Batch K-Means (SVD): Silhouette Score = 0.0919, Clusters = 4\n","Mini-Batch K-Means (tsne): Silhouette Score = 0.3704, Clusters = 4\n","K-Means (UMAP): Silhouette Score = 0.4861, Clusters = 4\n","K-Means (SVD): Silhouette Score = 0.0793, Clusters = 4\n","K-Means (tsne): Silhouette Score = 0.3865, Clusters = 4\n","DBSCAN (UMAP): Silhouette Score = -0.3241, Clusters = 36\n","DBSCAN (SVD): Silhouette Score = -0.0047, Clusters = 5\n","DBSCAN (tsne): Silhouette Score = 0.0729, Clusters = 4253\n"]}],"source":["from sklearn.metrics import silhouette_score\n","\n","# Function to safely compute silhouette score\n","def compute_silhouette(data, labels):\n","    unique_labels = len(set(labels)) - (1 if -1 in labels else 0)  # Exclude noise for DBSCAN\n","    if unique_labels > 1:  # Need at least 2 clusters for silhouette score\n","        return silhouette_score(data, labels)\n","    return None\n","\n","# Dictionary of methods and their data/labels\n","methods = {\n","    'Mini-Batch K-Means (UMAP)': (reduced_embeddings_umap, clusters_minibatch_umap),\n","    'Mini-Batch K-Means (SVD)': (reduced_embeddings_svd, clusters_minibatch_svd),\n","    'Mini-Batch K-Means (tsne)': (reduced_embeddings_tsne, clusters_minibatch_tsne),\n","    'K-Means (UMAP)': (reduced_embeddings_umap, clusters_kmeans_umap),\n","    'K-Means (SVD)': (reduced_embeddings_svd, clusters_kmeans_svd),\n","    'K-Means (tsne)': (reduced_embeddings_tsne, clusters_kmeans_tsne),\n","    'DBSCAN (UMAP)': (reduced_embeddings_umap, clusters_dbscan_umap),\n","    'DBSCAN (SVD)': (reduced_embeddings_svd, clusters_dbscan_svd),\n","    'DBSCAN (tsne)': (reduced_embeddings_tsne, clusters_dbscan_tsne),\n","    # 'Spectral Clustering (UMAP)': (reduced_embeddings_umap, clusters_spectral_umap),\n","    # 'Spectral Clustering (SVD)': (reduced_embeddings_svd, clusters_spectral_svd)\n","}\n","\n","# Compute and print silhouette scores\n","for method, (data, labels) in methods.items():\n","    score = compute_silhouette(data, labels)\n","    num_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n","    if score is not None:\n","        print(f\"{method}: Silhouette Score = {score:.4f}, Clusters = {num_clusters}\")\n","    else:\n","        print(f\"{method}: Cannot compute silhouette score (Clusters = {num_clusters})\")\n","\n","\n","#Mini-Batch K-Means (UMAP): Silhouette Score = 0.3382, Clusters = 4\n","# Mini-Batch K-Means (SVD): Silhouette Score = 0.0919, Clusters = 4\n","# K-Means (UMAP): Silhouette Score = 0.486  1, Clusters = 4\n","# K-Means (SVD): Silhouette Score = 0.0793, Clusters = 4\n","# DBSCAN (UMAP): Silhouette Score = -0.3241, Clusters = 36\n","# DBSCAN (SVD): Silhouette Score = -0.0047, Clusters = 5"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vi8WKyNs3hgk"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"enN3s1jwHMa_"},"outputs":[],"source":["from sklearn.cluster import KMeans\n","\n","# On UMAP-reduced data\n","kmeans_umap = KMeans(n_clusters=4, random_state=42)\n","clusters_kmeans_umap = kmeans_umap.fit_predict(reduced_embeddings_umap)\n","\n","# On SVD-reduced data\n","kmeans_svd = KMeans(n_clusters=4, random_state=42)\n","clusters_kmeans_svd = kmeans_svd.fit_predict(reduced_embeddings_svd)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q19gU6uSHMbA"},"outputs":[],"source":["from sklearn.cluster import DBSCAN\n","\n","# On UMAP-reduced data\n","dbscan_umap = DBSCAN(eps=0.5, min_samples=5)\n","clusters_dbscan_umap = dbscan_umap.fit_predict(reduced_embeddings_umap)\n","\n","# On SVD-reduced data\n","dbscan_svd = DBSCAN(eps=0.5, min_samples=5)\n","clusters_dbscan_svd = dbscan_svd.fit_predict(reduced_embeddings_svd)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xrQ7cRa_HMbA","outputId":"178068a3-4545-4907-a742-03b8ae78aea7"},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/ritikojha/Library/Python/3.9/lib/python/site-packages/sklearn/manifold/_spectral_embedding.py:329: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n","  warnings.warn(\n","/Users/ritikojha/Library/Python/3.9/lib/python/site-packages/sklearn/manifold/_spectral_embedding.py:329: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n","  warnings.warn(\n"]}],"source":["from sklearn.cluster import SpectralClustering\n","\n","# On UMAP-reduced data\n","spectral_umap = SpectralClustering(n_clusters=3, random_state=42, affinity='nearest_neighbors')\n","clusters_spectral_umap = spectral_umap.fit_predict(reduced_embeddings_umap)\n","\n","# On SVD-reduced data\n","spectral_svd = SpectralClustering(n_clusters=3, random_state=42, affinity='nearest_neighbors')\n","clusters_spectral_svd = spectral_svd.fit_predict(reduced_embeddings_svd)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":325512,"status":"ok","timestamp":1744957345494,"user":{"displayName":"Ritik Ojha","userId":"08390397347787036077"},"user_tz":240},"id":"Um7Zaj3DHMbA","outputId":"515c774a-8a58-42e4-c55e-130e381df3d0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mini-Batch K-Means (UMAP): Silhouette Score = 0.3382, Clusters = 4\n","Mini-Batch K-Means (SVD): Silhouette Score = 0.0919, Clusters = 4\n","K-Means (UMAP): Silhouette Score = 0.4861, Clusters = 4\n","K-Means (SVD): Silhouette Score = 0.0793, Clusters = 4\n","DBSCAN (UMAP): Silhouette Score = -0.3241, Clusters = 36\n","DBSCAN (SVD): Silhouette Score = -0.0047, Clusters = 5\n"]}],"source":["from sklearn.metrics import silhouette_score\n","\n","# Function to compute silhouette score safely\n","def compute_silhouette(data, labels):\n","    unique_labels = len(set(labels))\n","    if unique_labels > 1 and unique_labels < len(data):  # Valid clustering\n","        return silhouette_score(data, labels)\n","    return None\n","\n","# Dictionary of methods and their data/labels\n","methods = {\n","    'Mini-Batch K-Means (UMAP)': (reduced_embeddings_umap, clusters_minibatch_umap),\n","    'Mini-Batch K-Means (SVD)': (reduced_embeddings_svd, clusters_minibatch_svd),\n","    'K-Means (UMAP)': (reduced_embeddings_umap, clusters_kmeans_umap),\n","    'K-Means (SVD)': (reduced_embeddings_svd, clusters_kmeans_svd),\n","    'DBSCAN (UMAP)': (reduced_embeddings_umap, clusters_dbscan_umap),\n","    'DBSCAN (SVD)': (reduced_embeddings_svd, clusters_dbscan_svd),\n","    # 'Spectral Clustering (UMAP)': (reduced_embeddings_umap, clusters_spectral_umap),\n","    # 'Spectral Clustering (SVD)': (reduced_embeddings_svd, clusters_spectral_svd)\n","}\n","\n","# Compute and print silhouette scores\n","for method, (data, labels) in methods.items():\n","    score = compute_silhouette(data, labels)\n","    num_clusters = len(set(labels)) - (1 if -1 in labels else 0)  # Exclude noise for DBSCAN\n","    if score is not None:\n","        print(f\"{method}: Silhouette Score = {score:.4f}, Clusters = {num_clusters}\")\n","    else:\n","        print(f\"{method}: Cannot compute silhouette score (Clusters = {num_clusters})\")\n","\n","# Mini-Batch K-Means (UMAP): Silhouette Score = 0.3382, Clusters = 4\n","# Mini-Batch K-Means (SVD): Silhouette Score = 0.0919, Clusters = 4\n","# K-Means (UMAP): Silhouette Score = 0.4861, Clusters = 4\n","# K-Means (SVD): Silhouette Score = 0.0793, Clusters = 4\n","# DBSCAN (UMAP): Silhouette Score = -0.3241, Clusters = 36\n","# DBSCAN (SVD): Silhouette Score = -0.0047, Clusters = 5"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"89sTUI7kHMbA","outputId":"ac6d797a-6b1f-4a66-efe3-daf1d06a0d1e"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Cluster 0 (K-Means UMAP):\n","Example 1: lamentamos el inconveniente por favor siguenos en twitter para brindarte asistencia por mensajes directos\n","Example 2: hola ezequiel por favor siguenos en twitter para poder brindarte asistencia por mensajes directos\n","\n","Cluster 1 (K-Means UMAP):\n","Example 1: i would love the chance to review the account and provide assistance\n","Example 2: h there we would definitely like to work with you on this how long have you been eeriencing this issue\n","\n","Cluster 2 (K-Means UMAP):\n","Example 1: what information is incorrect\n","Example 2: no thank you\n"]}],"source":["# Add cluster labels to the subset\n","data_subset['kmeans_umap_cluster'] = clusters_kmeans_umap\n","\n","# Print 2 examples from each cluster\n","for cluster in range(3):\n","    print(f\"\\nCluster {cluster} (K-Means UMAP):\")\n","    cluster_data = data_subset[data_subset['kmeans_umap_cluster'] == cluster]['output'].head(2)\n","    for i, text in enumerate(cluster_data):\n","        print(f\"Example {i+1}: {text}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hkfV9P2JHMbA"},"outputs":[],"source":["# data_subset.to_csv('final_submission.csv', index = False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"},"widgets":{"application/vnd.jupyter.widget-state+json":{"04e7f398826944a5b6d1e902b8df831d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"09235973e4d046ec90982e02cbe5a8e0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a6c2286141d14050ab9b79fef1e354f1","IPY_MODEL_e47f66b021164d149d4f3e925e9741eb","IPY_MODEL_d192ebdfd02944538b7163b409386c43"],"layout":"IPY_MODEL_f7b08b27ea7f47e2b0efed3b128c8eb6"}},"1820f1a74f9c48ab9f28622ce516836c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1a00f4d394d3424a9fb1de404c9d8e19":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1b42788ff7354dc7b09069c72008b7ec":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e1742a1a4eb40879590c2b66e3baf1a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_528bd088204a46a48f64e1f23da09a30","max":48,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d03dc7000e6a4f309e89daa03a2a4c4e","value":48}},"2070db58e6624c44b8727de08f863cd6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2bad9f44bf874712abaa177db748184c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2d0cbd95bb9b42669a9ff3774badb466":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1b42788ff7354dc7b09069c72008b7ec","placeholder":"​","style":"IPY_MODEL_72fc63d3fe214a939995de1c7b5a838a","value":" 466k/466k [00:00&lt;00:00, 10.7MB/s]"}},"38fcd393a390457bbb3e755a32830466":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3c80dfc7caa44d64a4a21c2204929f9d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_953d3bcac0774397b841abc6d0d3787b","placeholder":"​","style":"IPY_MODEL_8eb993f0f1174fffbd10869bd406605e","value":" 48.0/48.0 [00:00&lt;00:00, 3.36kB/s]"}},"40e5d3e685c34e5196c52cbd462d6fd7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cbb007e930c64b24881815c2a7e11a82","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9a04f882947b49c6902688885f62c6b3","value":231508}},"47161eb6ea024fd797ed37a8c612b8d5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8dd5e7105a724ad79bfb49ac0adb78b1","placeholder":"​","style":"IPY_MODEL_be8a890fe826449c91d5dd82f1cc47f5","value":"vocab.txt: 100%"}},"48dffd59903a463283b8db0ee4dea8b5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"528bd088204a46a48f64e1f23da09a30":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"575c731ec8a44d8b8c28c7d471cd5403":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_47161eb6ea024fd797ed37a8c612b8d5","IPY_MODEL_40e5d3e685c34e5196c52cbd462d6fd7","IPY_MODEL_6c62d15389784891ac271b71366a003c"],"layout":"IPY_MODEL_2bad9f44bf874712abaa177db748184c"}},"5e6302875be544b9a25d2eb9e12a726c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6578b02a192a4564bc57810772b4bfbc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6cffe51a3f144cbe9c59a6d38d61486e","placeholder":"​","style":"IPY_MODEL_e28cd1ae603149bb8ad8612a0db01bfa","value":"tokenizer_config.json: 100%"}},"6c62d15389784891ac271b71366a003c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1a00f4d394d3424a9fb1de404c9d8e19","placeholder":"​","style":"IPY_MODEL_2070db58e6624c44b8727de08f863cd6","value":" 232k/232k [00:00&lt;00:00, 4.35MB/s]"}},"6cffe51a3f144cbe9c59a6d38d61486e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"72fc63d3fe214a939995de1c7b5a838a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"75b0c655fd8b49469e8d220ef0e7e2ec":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7d943efcc86841e2b4901b1d474fd436":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7de7680032784b2bad2506fbfe7daf2e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_967038c825554ffd814fe90ebb6bee30","placeholder":"​","style":"IPY_MODEL_7d943efcc86841e2b4901b1d474fd436","value":"tokenizer.json: 100%"}},"81983c0ec470445e82d1bb52f0dc98db":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_38fcd393a390457bbb3e755a32830466","max":466062,"min":0,"orientation":"horizontal","style":"IPY_MODEL_febab2bec880457a85bdcc0d2047d3e7","value":466062}},"8dd5e7105a724ad79bfb49ac0adb78b1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8e89198ae09242d9b8887fddb1a7f969":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7de7680032784b2bad2506fbfe7daf2e","IPY_MODEL_81983c0ec470445e82d1bb52f0dc98db","IPY_MODEL_2d0cbd95bb9b42669a9ff3774badb466"],"layout":"IPY_MODEL_5e6302875be544b9a25d2eb9e12a726c"}},"8eb993f0f1174fffbd10869bd406605e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"949fcb113a28446e899dbccd47bfa6a0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"953d3bcac0774397b841abc6d0d3787b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"967038c825554ffd814fe90ebb6bee30":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9a04f882947b49c6902688885f62c6b3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a38268f765514ac8a85b2f8d3281f91b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f39ab185245f4ea4b6800f33ea34d632","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_75b0c655fd8b49469e8d220ef0e7e2ec","value":570}},"a68ed39eeabf4a42b1c8a3213e4a9973":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a6c2286141d14050ab9b79fef1e354f1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d32cbe465c314025ab4bb9ea56680a60","placeholder":"​","style":"IPY_MODEL_f1a236102d0743ed904a8e2585484df7","value":"model.safetensors: 100%"}},"ab486fc72d4640d3b7fd20a794558069":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"afcb76ed830a4244a94a3d6b4448cdeb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f513a35e2825439bbbabd794b56987fa","placeholder":"​","style":"IPY_MODEL_ab486fc72d4640d3b7fd20a794558069","value":" 570/570 [00:00&lt;00:00, 26.7kB/s]"}},"be8a890fe826449c91d5dd82f1cc47f5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c2eff01113024168b02cc6eddd4c9f42":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c9a245d2c9e145e1aac36222cffb02c4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e736795d01c243c2bbeab4d951607547","IPY_MODEL_a38268f765514ac8a85b2f8d3281f91b","IPY_MODEL_afcb76ed830a4244a94a3d6b4448cdeb"],"layout":"IPY_MODEL_1820f1a74f9c48ab9f28622ce516836c"}},"cbb007e930c64b24881815c2a7e11a82":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d03dc7000e6a4f309e89daa03a2a4c4e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d192ebdfd02944538b7163b409386c43":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a68ed39eeabf4a42b1c8a3213e4a9973","placeholder":"​","style":"IPY_MODEL_ffd5fd6bd7584e5798af662d28026212","value":" 440M/440M [00:03&lt;00:00, 95.4MB/s]"}},"d32cbe465c314025ab4bb9ea56680a60":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"deb5f38e66834022856e1796de42b861":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6578b02a192a4564bc57810772b4bfbc","IPY_MODEL_1e1742a1a4eb40879590c2b66e3baf1a","IPY_MODEL_3c80dfc7caa44d64a4a21c2204929f9d"],"layout":"IPY_MODEL_48dffd59903a463283b8db0ee4dea8b5"}},"e28cd1ae603149bb8ad8612a0db01bfa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e47f66b021164d149d4f3e925e9741eb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_04e7f398826944a5b6d1e902b8df831d","max":440449768,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c2eff01113024168b02cc6eddd4c9f42","value":440449768}},"e736795d01c243c2bbeab4d951607547":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_949fcb113a28446e899dbccd47bfa6a0","placeholder":"​","style":"IPY_MODEL_faa69438909d4babaab78a1271505100","value":"config.json: 100%"}},"f1a236102d0743ed904a8e2585484df7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f39ab185245f4ea4b6800f33ea34d632":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f513a35e2825439bbbabd794b56987fa":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f7b08b27ea7f47e2b0efed3b128c8eb6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"faa69438909d4babaab78a1271505100":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"febab2bec880457a85bdcc0d2047d3e7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ffd5fd6bd7584e5798af662d28026212":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}
